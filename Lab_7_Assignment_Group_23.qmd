---
title: "Lab_7_Assignment_Group_23"
format:
  html:
    embed-resources: true
editor: visual
---

# **Lab 7 Assignment: Group 23**

# Lab 7: Collaborative Bio Data Science using GitHub via RStudio

**Group 23, members:**

-   Ana Pastor Mediavilla - s222761

-   Carlos de Santiago Le√≥n - s222766

-   Laura Figueiredo Tor - s222797

-   Monika Karolina Borkowska - s223203

-   Amanda Jimenez Lobato - s233150

## 1. Load libraries

```{r}
library('tidyverse')
library('broom')
library('patchwork')
library('ggrepel')

if(!require('cowplot')) {
  install.packages('cowplot')
  library('cowplot')
}
```

## 2. Load data

```{r}
#| message: false 
biopsy <- read_csv("https://wilkelab.org/classes/SDS348/data_sets/biopsy.csv")

biopsy
```

## 3. Analysis and Results

### 3.1. Data in PC coordinates

We fit first the PCA method according to the data by selecting the numeric features of the dataset and scaling them to have unit variance.

```{r}
pca_fit <- biopsy %>% 
  select(where(is.numeric)) %>% 
  prcomp(scale = TRUE)
```

```{r}
pca_fit <- biopsy %>% 
  select(where(is.numeric)) %>% 
  scale() %>% 
  prcomp()
```

Then, we reconstruct the data into the PC (principal components) and plot them, for analyzing possible relation/clustering according to the variable of interest (which in our study is the target/outcome related to the presence of a benign or malignant tumor).

```{r}
pca_fit %>%
  augment(biopsy) %>% 
  ggplot(aes(.fittedPC1, 
             .fittedPC2, 
             color = outcome)) + 
  geom_point(size = 1.5) +
  labs(title = "PCA scores",
       x = "Fitted PC1", 
       y = "Fitted PC2",
       color = "Outcome") + 
  scale_color_manual(values = c(malignant = "#D55E00", 
                                benign = "#0072B2")) +
  theme_half_open(12) + 
  background_grid() +
  theme(plot.title = element_text(hjust = 0.5))
```

From this PCA scores plot, we can observe a clear clustering according to the outcome variable by only looking at the two first principal components. This means that some numeric information from some of the data variables is shared between the two different outcomes. For example, benign data points can have similar values in variables like the uniform_cell_size, while the malignant data may have a different range.

This clear differential clustering between the two data labels suggests that the dataset could be applied for further studies and applications such as machine learning algorithms for cancer diagnosis.

### 3.2. Rotation matrix

We will proceed to extract the rotation matrix. The rotation matrix consist of a set of vectors which provide the direction of the principal components, therefore the rotation matrix contains the eigenvectors.

```{r}
pca_fit %>%
  tidy(matrix = "rotation")
```

The rotation matrix resulting from a PCA can be visualized in order to understand better how the variables contribute to the principal components and how these components are related to each other. This can be done by looking at the direction and length of the plotted arrows.

```{r}
# Arrow style for plotting
arrow_style <- arrow(
  angle = 20,
  ends = "first",
  type = "closed",
  length = grid::unit(8, "pt")
)

# Plot the rotation matrix by plotting the loadings

pca_fit %>%
  tidy(matrix = "rotation") %>%
  pivot_wider(names_from = "PC",
              names_prefix = "PC",
              values_from = "value") %>%
  ggplot(aes(PC1,
             PC2)) +
  geom_segment(xend = 0,
               yend = 0, 
               arrow = arrow_style,
               color = 'darkblue') +
  geom_text_repel(aes(label = column),
                  box.padding = 0.5,
                  segment.color = "darkgreen",
                  size = 4) +

  xlim(-1.25, .5) +
  ylim(-.5, 1) +
  coord_fixed() + # fix aspect ratio to 1:1
  theme_minimal_grid() +
  theme(plot.title =  element_text(hjust = 0.5, size = 24, face = "bold")) +
  labs(title = "PCA Rotation Matrix Plot", x = "PC1", y = "PC2")
```

From this plot we can see how many of the variables are closely related to each other, as is the case of `bare_nuclei` , `clump_thickness`, and `bland_chromatin`, where we can see how the rows have almost the same length and are pointing in the same direction. On the other hand, these variables are not related to `mitoses`, as the arrows are almost perpendicular (orthogonal), suggesting that they are uncorrelated. The length of the arrows represents the variance explained by each principal component, where longer arrows imply that the corresponding variable captures more variance of the data. This is the case with the variable `mitoses`, which captures a great amount of the variance. In the next section, we do a further analysis on the variance explained by each component can be done in order to identify which components capture more variety of data.

### 3.3. Variance

Finally, we calculate the variance explained by each PC, to measure how much of the total variance is captured by each principal component. For such purpose, we use the `tidy()` function from broom.

```{r}
pca_fit %>%
  tidy(matrix = "eigenvalues")
```

In the table, we can see that PC1 captures an ashtoningly high percentage of the total variance: 65.56 %, more than half. PC2 accounts for 8.62% of the variance, what implies that just by plotting the first 2 PCs, we are representing 74,17 % of the variance of the data.

By looking at the cumulative variance, we can see that 6 PCs are enough to cover 90% of the variance, and that 9 PCs are required to represent 100% of the variance.

To obtain a visual representation of this, we can plot the variance explained by each PC.

```{r}

pca_fit %>%
  tidy(matrix = "eigenvalues") %>%
  ggplot(aes(PC, percent)) +
  geom_col(fill = "#56B4E9", alpha = 0.8) +
  scale_x_continuous(breaks = 1:9) +
  scale_y_continuous(
    labels = scales::percent_format(),
    expand = expansion(mult = c(0, 0.01))
  ) +
  theme_minimal_hgrid(12)
```

This graphs shows how PC1 accounts for a lot more variance than the rest of the PCs, and that the difference between PC2 and the rest of the PCs is not so high. Furthermore, the variance explained by PC9 is remarkably low (0.982%).

## 4. Discussion

In this micro-report, we applied Principal Component Analysis (PCA) to the breast cancer dataset obtained from the University of Wisconsin Hospitals, Madison. The analysis, focusing on the first two principal components, revealed a clear separation between benign and malignant tumors. This observation suggests that the numerical attributes in the dataset hold valuable information that could be used by machine learning algorithms for cancer diagnosis.

Moreover, the PCA rotation matrix allowed us to see how much each varaible contributed to the two first PCs, and how they were correlated or uncorrelated to one another.

Finally, the calculation of the variance explained by each PC validated the previous work, since the two first PCs (the PCs we had been studying), accounted for a high amount of the total variance: 74.17%.
